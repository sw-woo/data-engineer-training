{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 7. Optimizing and Tuning Spark Applications\n",
    "> 6장에서는 스파크가 어떻게 메모리 관리를 하고, 고급 API 를 통해서 데이터셋을 구성하는 지에 대해 학습했으며, 이번 장에서는 최적화를 위한 스파크 설정과, 조인 전략들을 살펴보고, 스파크 UI 를 통해 안좋은 영향을 줄 수 있는 것들에 대한 힌트를 얻고자 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.session.timeZone\", \"Asia/Seoul\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7.1 Optimizing and Tuning Spark for Efficiency\n",
    "> 스파크는 [튜닝](https://spark.apache.org/docs/latest/tuning.html)을 위한 다양한 설정을 제공하며, [설정](https://spark.apache.org/docs/latest/configuration.html)값을 통해 확인할 수 있습니다\n",
    "\n",
    "### 7.1.1 Viewing and Setting Apache Spark Configurations\n",
    "> 아래의 순서대로 스파크는 설정값을 읽어들이며, 가장 마지막에 변경된 값이 반영됩니다\n",
    "\n",
    "#### 1. 설치된 스파크 경로의 conf/spark-default.conf 파일을 생성 및 수정\n",
    "\n",
    "#### 2. 스파크 실행 시에 옵션을 지정하는 방법\n",
    "```bash\n",
    "$ spark-submit --conf spark.sql.shuffle.partitions=5 --conf \"spark.executor.memory=2g\" --class main.scala.chapter7.SparkConfig_7_1 jars/mainscala-chapter7_2.12-1.0.jar\n",
    "```\n",
    "\n",
    "#### 3. 스파크 코드 내에서 직접 지정하는 방법\n",
    "```scala\n",
    "SparkSession.builder\n",
    ".config(\"spark.sql.shuffle.partitions\", 5)\n",
    ".config(\"spark.executor.memroy\", \"2g\")\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.app.id', 'local-1619576347079')\n",
      "('spark.app.name', 'pyspark-shell')\n",
      "('spark.driver.host', 'ddd9c0cd39a3')\n",
      "('spark.driver.port', '42483')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.master', 'local[*]')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.sql.session.timeZone', 'Asia/Seoul')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.ui.showConsoleProgress', 'true')\n"
     ]
    }
   ],
   "source": [
    "# 파이스파크 내에서는 sparkContext 를 통해서 해당 정보를 가져올 수 있습니다\n",
    "def printConfigs(session):\n",
    "    for x in sorted(session.sparkContext.getConf().getAll()):\n",
    "        print(x)\n",
    "\n",
    "printConfigs(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------+----------------------------------------------------------------+\n",
      "|key                                                      |value                                                           |\n",
      "+---------------------------------------------------------+----------------------------------------------------------------+\n",
      "|spark.sql.adaptive.advisoryPartitionSizeInBytes          |<value of spark.sql.adaptive.shuffle.targetPostShuffleInputSize>|\n",
      "|spark.sql.adaptive.coalescePartitions.enabled            |true                                                            |\n",
      "|spark.sql.adaptive.coalescePartitions.initialPartitionNum|<undefined>                                                     |\n",
      "|spark.sql.adaptive.coalescePartitions.minPartitionNum    |<undefined>                                                     |\n",
      "|spark.sql.adaptive.enabled                               |false                                                           |\n",
      "+---------------------------------------------------------+----------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SparkSQL의 경우 내부적으로 사용되는 설정값이 다르기 때문에 더 많은 정보가 출력됩니다\n",
    "spark.sql(\"SET -v\").select(\"key\", \"value\").where(\"key like '%spark.sql%'\").show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 스파크 UI 를 통해서도 확인이 가능합니다\n",
    "\n",
    "![spakr-ui](images/spark-ui.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 5\n"
     ]
    }
   ],
   "source": [
    "# 스파크 기본 설정 spark.sql.shuffle.partitions 값을 확인하고, 프로그램 상에서 변경 후 테스트 합니다\n",
    "num_partitions = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "mod_partitions = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", num_partitions)\n",
    "print(num_partitions, mod_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.2 Scaling Spark for Large Workloads\n",
    "\n",
    "#### 1. 정적 vs 동적 리소스 할당의 선택\n",
    "> CPU 및 Memory 사용을 애플리케이션에 따라 지정하는 정적 리소스 할당과 동적 리소스 할당은 처리해야 할 데이터의 특성에 따라 선택할 수 있으며, 환경설정을 다르게 구성해야 합니다.\n",
    "\n",
    "* 데이터의 크기가 일정하지 않고, 유동적\n",
    "* 특히 데이터의 크기가 고르지 않은 스트리밍 처리\n",
    "* 멀티테넌시 환경의 분석용 클러스터의 데이터 리소스 관리\n",
    "\n",
    "#### 2. 동적 리소스 할당 설정 가이드\n",
    "* 기본 설정은 false 이므로 아래의 값들에 대한 설정이 별도로 되어야 하며, REPL 환경에서 지원하지 않는 값들도 존재하므로, 프로그램을 통한 수정이 필요합니다\n",
    "```\n",
    "spark.dynamicAllocation.enabled true\n",
    "spark.dynamicAllocation.minExecutors 2\n",
    "spark.dynamicAllocation.schedulerBacklogTimeout 1m\n",
    "spark.dynamicAllocation.maxExecutors 20\n",
    "spark.dynamicAllocation.executorIdleTimeout 2min\n",
    "```\n",
    "* 아래의 과정을 통해 동적 리소스를 관리합니다\n",
    "  - 1. 스파크 드라이버가 클러스터 매니저에 2개(minExecutors)의 익스큐터를 요청합니다\n",
    "  - 2. 작업 큐의 백로그가 증가하여, 백로그 타임아웃(schedulerBacklogTimeout)이 발생하는 경우 새로운 익스큐터 요청이 발생합니다\n",
    "  - 3. 스케줄링 된 작업들이 1분 이상 지연되는 경우 드라이버는 새로운 익스큐터를 최대 20개(maxExecutors) 까지 요청합니다\n",
    "  - 4. 스파크 드라이버는 2분 이상 (executorIdleTimeout) 작업이 할당되지 않는 익스큐터 들을 종료시킵니다\n",
    "\n",
    "#### 3. 스파크 익스큐터의 메모리와 셔플 서비스의 설정 가이드\n",
    "![external-memory-layout](images/external-memory-layout.png)\n",
    "* 맵, 스필 그리고 병합 프로세스들이 I/O 부족에 따른 문제점을 갖지 않으며, 최종 셔플 파티션이 디스크에 저장되기 전에 버퍼 메모리를 확보할 수 있도록 설정을 아래와 같이 조정할 수 있습니다\n",
    "![spark-conf-io](images/spark-conf-io.png)\n",
    "\n",
    "#### 4. 스파크 병렬성을 최대화\n",
    "> 스파크가 데이터를 어떻게 저장소로부터 메모리에 적재하는지, 스파크에 있어서 파티션이 어떻게 활용되는지를 이해해야 합니다\n",
    "\n",
    "* 매 스테이지 마다 많은 타스크들이 존재하지만, 스파크는 기껏해야 코어당 작업당 하나의 스레드만 할당하며, 개별 타스크는 독립된 파티션 하나를 처리합니다.\n",
    "* 리소스 사용을 최적화하고, 병렬성을 최대화 하려면 익스큐터에 존재하는 코어수들 만큼 많은 파티션들이 존재해야 합니다. (유휴 코어를 두지 않기 위함)\n",
    "![figure.7-3](images/figure.7-3.png)\n",
    "\n",
    "\n",
    "#### 5. 파티션은 구성에 대한 이해와 재구성\n",
    "* 분산 저장소에 저장시에 구성되는 경우\n",
    "  - HDFS, S3 등의 저장소의 기본 파일블록의 크기는 64mb, 128mb 이며, 파일 크기가 작고 많아질 수록 파티션당 할당해야 하는 코어수가 모자라기 때문에 \"small file problem\" 을 피해야 합니다\n",
    "* 스파크의 셔플링을 통해 생성되는 경우\n",
    "  - 집계함수나 조인과 같은 Wide Transformation 과정에서 셔플링이 발생 (Network & Disk I/O 비용)\n",
    "  - 기본 셔플 파티션 수는 200개인데 작은 데이터집합이나, 스트리밍 워크로드 등에는 **충분히 많은 수이기 때문에 조정이 필요**합니다\n",
    "  - 최종 결과 테이블의 용량 및 사용 용도에 따라 의도적인 파티션 수를 조정할 수 있습니다 (repartition, coalesce)\n",
    "\n",
    "#### 질문과 답변\n",
    "* 대부분 dynamic allocation 을 쓰면 좋을거 아닐까?\n",
    "  - 워크로드가 예상된다면 동적할당은 필요없는 리소스 및 관리 비용이 더 들어가기 때문에 성능에 영향을 줄 수 있습니다\n",
    "* REPL 이 뭔가?\n",
    "  - Read-Evaluate-Print Loop 의 약자\n",
    "* dynamic allocation 은 수시로 변경할 수 없는가? 왜 그런가?\n",
    "* off-heap 이 좋으면 모두 off-heap 사용하지 왜 jvm 메모리를 이렇게나 많이 사용하는가?\n",
    "  - 자바에서 사용하는 구조화된 API의 장점과 네이티브 라이브러리의 데이터 송수신 및 읽고 쓰기의 장점을 모두 취하기 위함\n",
    "* execution vs storage 메모리의 비율을 어떻게 확인할 수 있는가? 오히려 삽질 아닌가?\n",
    "  - 직접 셋팅하기 보다는 관련 옵션을 조정하면서 튜닝합니다\n",
    "* spark 작업에서의 spill 절차는 무엇이고 왜 발생하며 어떻게 해결할 수 있는가?\n",
    "  - 스파크 익스큐터가 위의 각 레이어에 할당된 메모리를 모두 사용한 경우 디스크로 저장하는 경우를 Spill 이라고 합니다\n",
    "  - Disk I/O 는 성능에 큰 영향을 미치기 때문에 SSD 를 사용한다면 좋은 성능을 효과를 기대할 수 있습니다\n",
    "```\n",
    "operations, the shuffle will spill results to executors’ local disks at the location specified in spark.local.directory. Having performant SSD disks for this operation will boost the performance.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numDF = spark.range(1000).repartition(16)\n",
    "numDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Caching and Persistence of Data\n",
    "> cache() 와 persist() 는 거의 동일하지만, persist() 의 경우 persistent level 을 결정할 수 있습니다 (메모리, 디스크, 직렬화, 비직렬화 등)\n",
    "\n",
    "### 7.2.1 DataFrame.cache()\n",
    "* DataFrame 은 부분적으로 캐시가 가능하지만, 파티션은 그렇지 못 합니다. 예를 들어 8개의 파티션 중 4.5개 정도를 사용할 메모리가 있는 경우 4개의 파티션만 캐시됩니다\n",
    "  - 캐시되지 않은 데이터를 읽는 데에는 문제가 없지만, 모두 다시 계산되어야 하는 비용이 발생합니다\n",
    "* cache() 혹은 persist() 호출 시에 DataFrame 은 take(1) 같은 경우 첫 번째 파티션만 캐싱이 이루어지고, count() 같은 action 수행 시에는 모든 데이터가 캐싱이 된다는 점을 알고 있어야 합니다\n",
    "  - rdd.cache()는 persist(StorageLevel.MEMORY_ONLY) 로 \n",
    "  - df.cache()는 persist(StorageLevel.MEMORY_AND_DISK) 로 동작합니다\n",
    "\n",
    "![persist-storage](images/persist-storage.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed number is 18\n",
      "3.8046398162841797\n"
     ]
    }
   ],
   "source": [
    "# 반복적으로 수행하는 경우 노트북 프로그램의 캐싱될 수 있기 때문에, 매번 다른 프로그램 수행을 위해서 랜덤 시드숫자를 매번 더해줍니다.\n",
    "import random\n",
    "seed = random.randint(1,100)\n",
    "print(\"seed number is {}\".format(seed))\n",
    "cached = spark.range(10 * 1000 * 1000 + seed).toDF(\"id\").withColumn(\"square\", expr(\"id * id\"))\n",
    "import time\n",
    "start = time.time()\n",
    "cached.cache() # 데이터를 캐싱\n",
    "cached.count() # Materialize the cache\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1505875587463379\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "cached.count()\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2 DataFrame.persist()\n",
    "![table.7-2](images/table.7-2.png)\n",
    "![figure.7-5](images/figure.7-5.png)\n",
    "\n",
    "* 테이블 캐시를 사용하는 경우도 cache() 와 동일한 결과를 보여줍니다\n",
    "![figure.7-5-1](images/figure.7-5-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed number is 37\n",
      "2.6302409172058105\n"
     ]
    }
   ],
   "source": [
    "# 반복적으로 수행하는 경우 노트북 프로그램의 캐싱될 수 있기 때문에, 매번 다른 프로그램 수행을 위해서 랜덤 시드숫자를 매번 더해줍니다.\n",
    "import random\n",
    "seed = random.randint(1,100)\n",
    "print(\"seed number is {}\".format(seed))\n",
    "persisted = spark.range(10 * 1000 * 1000 + seed).toDF(\"id\").withColumn(\"square\", expr(\"id * id\"))\n",
    "import time\n",
    "start = time.time()\n",
    "from pyspark import StorageLevel\n",
    "persisted.persist(StorageLevel.DISK_ONLY) # 데이터를 캐싱\n",
    "persisted.count() # Materialize the cache\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19655108451843262\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "persisted.count()\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed number is 39\n",
      "2.4470584392547607\n"
     ]
    }
   ],
   "source": [
    "# 반복적으로 수행하는 경우 노트북 프로그램의 캐싱될 수 있기 때문에, 매번 다른 프로그램 수행을 위해서 랜덤 시드숫자를 매번 더해줍니다.\n",
    "import random\n",
    "seed = random.randint(1,100)\n",
    "print(\"seed number is {}\".format(seed))\n",
    "table_cached = spark.range(10 * 1000 * 1000 + seed).toDF(\"id\").withColumn(\"square\", expr(\"id * id\"))\n",
    "table_cached.createOrReplaceTempView(\"square\")\n",
    "import time\n",
    "start = time.time()\n",
    "spark.sql(\"CACHE TABLE square\") # 데이터를 캐싱\n",
    "spark.sql(\"SELECT COUNT(1) FROM square\") # Materialize the cache\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02222752571105957\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "spark.sql(\"SELECT COUNT(1) FROM square\")\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.3 When to Cache and Persist\n",
    "> 대용량 테이블을 자주 쿼리하는 경우 혹은 변환에 활용되는 경우에 사용합니다\n",
    "* 기계학습 훈련 시와 같이 반복 적인 데이터프레임의 조회\n",
    "* ETL 데이터 파이프라인의 변환작업에 빈번하게 사용되는 공통 테이블의 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.4 When Not to Cache and Persist\n",
    "> 너무 크거나, 자주 사용되지 않는 테이블의 경우는 지양합니다. 왜냐하면 데이터의 직렬화, 역직렬화에 따른 비용이 상당하기 때문에 오히려 전체적인 처리시간에 악영향을 줄 수 있습니다.\n",
    "* 메모리에 들어가지 않을 만큼 큰 데이터\n",
    "* 크기에 비해서 자주 사용되지 않는 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7.3 A Family of Spark Joins\n",
    "\n",
    "```text\n",
    "/**\n",
    "   * Select the proper physical plan for join based on join strategy hints, the availability of\n",
    "   * equi-join keys and the sizes of joining relations. Below are the existing join strategies,\n",
    "   * their characteristics and their limitations.\n",
    "   *\n",
    "   * - Broadcast hash join (BHJ):\n",
    "   *     Only supported for equi-joins, while the join keys do not need to be sortable.\n",
    "   *     Supported for all join types except full outer joins.\n",
    "   *     BHJ usually performs faster than the other join algorithms when the broadcast side is\n",
    "   *     small. However, broadcasting tables is a network-intensive operation and it could cause\n",
    "   *     OOM or perform badly in some cases, especially when the build/broadcast side is big.\n",
    "   *\n",
    "   * - Shuffle hash join:\n",
    "   *     Only supported for equi-joins, while the join keys do not need to be sortable.\n",
    "   *     Supported for all join types.\n",
    "   *     Building hash map from table is a memory-intensive operation and it could cause OOM\n",
    "   *     when the build side is big.\n",
    "   *\n",
    "   * - Shuffle sort merge join (SMJ):\n",
    "   *     Only supported for equi-joins and the join keys have to be sortable.\n",
    "   *     Supported for all join types.\n",
    "   *\n",
    "   * - Broadcast nested loop join (BNLJ):\n",
    "   *     Supports both equi-joins and non-equi-joins.\n",
    "   *     Supports all the join types, but the implementation is optimized for:\n",
    "   *       1) broadcasting the left side in a right outer join;\n",
    "   *       2) broadcasting the right side in a left outer, left semi, left anti or existence join;\n",
    "   *       3) broadcasting either side in an inner-like join.\n",
    "   *     For other cases, we need to scan the data multiple times, which can be rather slow.\n",
    "   *\n",
    "   * - Shuffle-and-replicate nested loop join (a.k.a. cartesian product join):\n",
    "   *     Supports both equi-joins and non-equi-joins.\n",
    "   *     Supports only inner like joins.\n",
    "   */\n",
    "```\n",
    "### 7.3.1 Broadcast Hash Join\n",
    "> 드라이버 혹은 익스큐터의 메모리 보다 충분히 작은 경우에 해당 데이터를 broadcast 변수에 담아, 상대적으로 큰 데이터가 존재하는 노드로 변수를 전달하기 때문에 map 단계에서 join 이 일어나게 되어 *map-side-join* 이라고 부르며, 조인 성능에 가장 큰 영향을 미치는 셔플이 발생하지 않게 되어 성능이 좋습니다.\n",
    "\n",
    "#### When to use a broadcast hash join\n",
    "* 작고 큰 데이터 집합의 개별 키가 스파크에 의해서 같은 파티션에 해시되어 있는 경우\n",
    "  - 버킷 등을 통해 이미 동일한 노드에 저장되어 있는 경우로 추측\n",
    "  - hash-join 과 같이 hash table 을 사용하는 것처럼 보이지는 않으나 확인이 필요함\n",
    "* 하나의 데이터 집합이 다른 데이터 집합에 비해 훨씬 작을 때 (그리고 기본 구성 메모리가 충분한 경우 10MB 이상)\n",
    "* 정렬되지 않은 키들의 매칭을 기반으로 두 데이터집합 들을 결합하기 위해서, 동등 조인을 수행하기를 워한는 경우\n",
    "  - 해시 조인이기 때문에 정렬되지 않은 상태의 Equi-join 이 가능하기 때문\n",
    "  - [non equi-join](https://www.essentialsql.com/non-equi-join-sql-purpose/) 은 anti-join 혹은 range-join 이 있다\n",
    "* 모든 스파크 익스큐터들에 작은 데이터가 브로드캐스트 될 것이 명확해서, 네트워크 밴드나 OOM 오류를 걱정할 필요가 없을때\n",
    "\n",
    "#### spark.sql.autoBroadcastJoinThreshold 값으로 설정을 변경할 수 있으며, 기본 값은 10m 입니다\n",
    "![figure.7-6](images/figure.7-6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|name  |type|\n",
      "+------+----+\n",
      "|Cat   |1   |\n",
      "|Dog   |1   |\n",
      "|Monkey|2   |\n",
      "|Lion  |3   |\n",
      "|Tiger |3   |\n",
      "+------+----+\n",
      "\n",
      "+------+--------+\n",
      "|  name|category|\n",
      "+------+--------+\n",
      "|   Cat|     Fat|\n",
      "|   Dog|     Fat|\n",
      "|  Lion|   Beast|\n",
      "| Tiger|   Beast|\n",
      "|Monkey|  Animal|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "animal = spark.createDataFrame([(\"Cat\", 1), (\"Dog\", 1), (\"Monkey\", 2), (\"Lion\", 3), (\"Tiger\", 3)], [\"name\", \"type\"])\n",
    "animal.show(truncate=False)\n",
    "category = spark.createDataFrame([(\"Fat\", 1), (\"Animal\", 2), (\"Beast\", 3)], [\"category\", \"id\"])\n",
    "animal.join(category, animal.type == category.id, \"left_outer\").select(\"name\", \"category\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|category|\n",
      "+------+--------+\n",
      "|   Cat|     Fat|\n",
      "|   Dog|     Fat|\n",
      "|Monkey|  Animal|\n",
      "|  Lion|   Beast|\n",
      "| Tiger|   Beast|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "animal.join(broadcast(category), animal.type == category.id, \"left_outer\").select(\"name\", \"category\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 스파크 3.0 에서 추가된 기능으로 explain 모드를 입력할 수 있으며 simple, extended, codegen, cost, formatted 등의 옵션을 제공합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "SortMergeJoin [type#282L], [id#295L], LeftOuter\n",
      ":- *(2) Sort [type#282L ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(type#282L, 200), true, [id=#277]\n",
      ":     +- *(1) Scan ExistingRDD[name#281,type#282L]\n",
      "+- *(4) Sort [id#295L ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(id#295L, 200), true, [id=#282]\n",
      "      +- *(3) Filter isnotnull(id#295L)\n",
      "         +- *(3) Scan ExistingRDD[category#294,id#295L]\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "* BroadcastHashJoin LeftOuter BuildRight (5)\n",
      ":- * Scan ExistingRDD (1)\n",
      "+- BroadcastExchange (4)\n",
      "   +- * Filter (3)\n",
      "      +- * Scan ExistingRDD (2)\n",
      "\n",
      "\n",
      "(1) Scan ExistingRDD [codegen id : 2]\n",
      "Output [2]: [name#281, type#282L]\n",
      "Arguments: [name#281, type#282L], MapPartitionsRDD[65] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "(2) Scan ExistingRDD [codegen id : 1]\n",
      "Output [2]: [category#294, id#295L]\n",
      "Arguments: [category#294, id#295L], MapPartitionsRDD[72] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "(3) Filter [codegen id : 1]\n",
      "Input [2]: [category#294, id#295L]\n",
      "Condition : isnotnull(id#295L)\n",
      "\n",
      "(4) BroadcastExchange\n",
      "Input [2]: [category#294, id#295L]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[1, bigint, false])), [id=#305]\n",
      "\n",
      "(5) BroadcastHashJoin [codegen id : 2]\n",
      "Left keys [1]: [type#282L]\n",
      "Right keys [1]: [id#295L]\n",
      "Join condition: None\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "animal.join(category, animal.type == category.id, \"left_outer\").explain(\"simple\")\n",
    "animal.join(broadcast(category), animal.type == category.id, \"left_outer\").explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 소트머지조인 | 브로드캐스트 조인 |\n",
    "| --- | --- |\n",
    "| ![join_shuffle](images/join_shuffle.png) | ![join_broadcast](images/join_broadcast.png) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.2 Shuffle Sort Merge Join\n",
    "> 두개의 대용량 데이터 집합을 조인하는 가장 효과적인 알고리즘이며, 기본 설정은 spark.sql.join.preferSortMergeJoin 은 enabled 된 상태입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreashold\", \"10485760b\") # default value\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreashold\", \"-1\") # force sortMergeJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|  name|type|\n",
      "+------+----+\n",
      "|   Cat|   1|\n",
      "|   Dog|   1|\n",
      "|Monkey|   2|\n",
      "|  Lion|   3|\n",
      "| Tiger|   3|\n",
      "+------+----+\n",
      "\n",
      "+---+-----+\n",
      "| id| item|\n",
      "+---+-----+\n",
      "|  0|SKU-0|\n",
      "|  1|SKU-1|\n",
      "|  2|SKU-2|\n",
      "|  3|SKU-3|\n",
      "|  4|SKU-4|\n",
      "|  5|SKU-5|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "states = spark.createDataFrame([(0, \"AZ\"), (1, \"CO\"), (3, \"TX\"), (4, \"N\"), (5, \"MI\")], [\"id\", \"state\"])\n",
    "items = spark.createDataFrame([(0, \"SKU-0\"), (1, \"SKU-1\"), (2, \"SKU-2\"), (3, \"SKU-3\"), (4, \"SKU-4\"), (5, \"SKU-5\")], [\"id\", \"item\"])\n",
    "animal.show()\n",
    "items.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+-------+-----+-----+----+---------+------------------------+----------+\n",
      "|transaction_id|quantity|users_id|amount |state|items|uid |login    |email                   |user_state|\n",
      "+--------------+--------+--------+-------+-----+-----+----+---------+------------------------+----------+\n",
      "|8590          |8590    |964     |17180.0|AZ   |SKU-5|964 |user_964 |user_964@databricks.com |TX        |\n",
      "|4464          |4464    |1677    |8928.0 |NY   |SKU-2|1677|user_1677|user_1677@databricks.com|CO        |\n",
      "|9042          |9042    |1677    |18084.0|CO   |SKU-1|1677|user_1677|user_1677@databricks.com|CO        |\n",
      "|5549          |5549    |1697    |11098.0|MI   |SKU-5|1697|user_1697|user_1697@databricks.com|CO        |\n",
      "|7950          |7950    |1806    |15900.0|MI   |SKU-3|1806|user_1806|user_1806@databricks.com|MI        |\n",
      "|2924          |2924    |2040    |5848.0 |CA   |SKU-1|2040|user_2040|user_2040@databricks.com|TX        |\n",
      "|8071          |8071    |2040    |16142.0|CA   |SKU-3|2040|user_2040|user_2040@databricks.com|TX        |\n",
      "|4897          |4897    |2250    |9794.0 |TX   |SKU-3|2250|user_2250|user_2250@databricks.com|NY        |\n",
      "|429           |429     |2453    |858.0  |CO   |SKU-5|2453|user_2453|user_2453@databricks.com|CA        |\n",
      "|8226          |8226    |2453    |16452.0|TX   |SKU-1|2453|user_2453|user_2453@databricks.com|CA        |\n",
      "|64            |64      |2529    |128.0  |TX   |SKU-0|2529|user_2529|user_2529@databricks.com|TX        |\n",
      "|3499          |3499    |2529    |6998.0 |TX   |SKU-3|2529|user_2529|user_2529@databricks.com|TX        |\n",
      "|6458          |6458    |2529    |12916.0|CO   |SKU-2|2529|user_2529|user_2529@databricks.com|TX        |\n",
      "|1617          |1617    |2927    |3234.0 |TX   |SKU-5|2927|user_2927|user_2927@databricks.com|CA        |\n",
      "|6750          |6750    |2927    |13500.0|TX   |SKU-2|2927|user_2927|user_2927@databricks.com|CA        |\n",
      "|4110          |4110    |5385    |8220.0 |CA   |SKU-5|5385|user_5385|user_5385@databricks.com|CA        |\n",
      "|7004          |7004    |5385    |14008.0|CO   |SKU-1|5385|user_5385|user_5385@databricks.com|CA        |\n",
      "|2376          |2376    |5409    |4752.0 |NY   |SKU-5|5409|user_5409|user_5409@databricks.com|CO        |\n",
      "|3346          |3346    |5556    |6692.0 |NY   |SKU-3|5556|user_5556|user_5556@databricks.com|NY        |\n",
      "|4109          |4109    |5556    |8218.0 |CO   |SKU-1|5556|user_5556|user_5556@databricks.com|NY        |\n",
      "+--------------+--------+--------+-------+-----+-----+----+---------+------------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreashold\", \"-1\") # force sortMergeJoin\n",
    "\n",
    "states = {0:\"AZ\", 1:\"CO\", 2:\"CA\", 3: \"TX\", 4: \"NY\", 5:\"MI\"}\n",
    "items = {0:\"SKU-0\", 1:\"SKU-1\", 2:\"SKU-2\", 3: \"SKU-3\", 4: \"SKU-4\", 5:\"SKU-5\"}\n",
    "\n",
    "usersDF = spark.range(0, 10000).rdd.map(lambda id: (id[0], \"user_{}\".format(id[0]), \"user_{}@databricks.com\".format(id[0]), states[random.randint(0, 5)])).toDF([\"uid\", \"login\", \"email\", \"user_state\"])\n",
    "ordersDF = spark.range(0, 10000).rdd.map(lambda r: (r[0], r[0], random.randint(0, 10000), 10 * r[0] * 0.2, states[random.randint(0, 5)], items[random.randint(0,5)])).toDF([\"transaction_id\", \"quantity\", \"users_id\", \"amount\", \"state\", \"items\"])\n",
    "\n",
    "# usersDF.show(truncate=False)\n",
    "# ordersDF.show(truncate=False)\n",
    "\n",
    "usersOrdersDF = ordersDF.join(usersDF, ordersDF.users_id == usersDF.uid)\n",
    "usersOrdersDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "* SortMergeJoin Inner (9)\n",
      ":- * Sort (4)\n",
      ":  +- Exchange (3)\n",
      ":     +- * Filter (2)\n",
      ":        +- * Scan ExistingRDD (1)\n",
      "+- * Sort (8)\n",
      "   +- Exchange (7)\n",
      "      +- * Filter (6)\n",
      "         +- * Scan ExistingRDD (5)\n",
      "\n",
      "\n",
      "(1) Scan ExistingRDD [codegen id : 1]\n",
      "Output [6]: [transaction_id#406L, quantity#407L, users_id#408L, amount#409, state#410, items#411]\n",
      "Arguments: [transaction_id#406L, quantity#407L, users_id#408L, amount#409, state#410, items#411], MapPartitionsRDD[123] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "(2) Filter [codegen id : 1]\n",
      "Input [6]: [transaction_id#406L, quantity#407L, users_id#408L, amount#409, state#410, items#411]\n",
      "Condition : isnotnull(users_id#408L)\n",
      "\n",
      "(3) Exchange\n",
      "Input [6]: [transaction_id#406L, quantity#407L, users_id#408L, amount#409, state#410, items#411]\n",
      "Arguments: hashpartitioning(users_id#408L, 200), true, [id=#417]\n",
      "\n",
      "(4) Sort [codegen id : 2]\n",
      "Input [6]: [transaction_id#406L, quantity#407L, users_id#408L, amount#409, state#410, items#411]\n",
      "Arguments: [users_id#408L ASC NULLS FIRST], false, 0\n",
      "\n",
      "(5) Scan ExistingRDD [codegen id : 3]\n",
      "Output [4]: [uid#396L, login#397, email#398, user_state#399]\n",
      "Arguments: [uid#396L, login#397, email#398, user_state#399], MapPartitionsRDD[112] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0, ExistingRDD, UnknownPartitioning(0)\n",
      "\n",
      "(6) Filter [codegen id : 3]\n",
      "Input [4]: [uid#396L, login#397, email#398, user_state#399]\n",
      "Condition : isnotnull(uid#396L)\n",
      "\n",
      "(7) Exchange\n",
      "Input [4]: [uid#396L, login#397, email#398, user_state#399]\n",
      "Arguments: hashpartitioning(uid#396L, 200), true, [id=#423]\n",
      "\n",
      "(8) Sort [codegen id : 4]\n",
      "Input [4]: [uid#396L, login#397, email#398, user_state#399]\n",
      "Arguments: [uid#396L ASC NULLS FIRST], false, 0\n",
      "\n",
      "(9) SortMergeJoin [codegen id : 5]\n",
      "Left keys [1]: [users_id#408L]\n",
      "Right keys [1]: [uid#396L]\n",
      "Join condition: None\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersOrdersDF.explain(\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizing the shuffle sort merge join\n",
    "> Sort-Merge 조인의 가장 큰 비용인 Exchange Stage 를 제거하여 성능향상을 도모할 수 있습니다. 이는 버킷을 통해 해당 데이터를 생성하는 시점에 미리 정렬해 두는 접근입니다. 즉 자주 사용되는 equi-join 의 컬럼을 기준으로 버킷 수준에서 정렬해둔다고 보시면 됩니다.\n",
    "\n",
    "![join_exchange](images/join_exchange.png)\n",
    "![join_bucket](images/join_bucket.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\",\"true\")\n",
    "%rm -rf \"spark-warehouse/userstbl\"\n",
    "%rm -rf \"spark-warehouse/orderstbl\"\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "(\n",
    "    usersDF.orderBy(asc(\"uid\"))\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"parquet\")\n",
    "    .bucketBy(8, \"uid\")\n",
    "    .saveAsTable(\"UsersTbl\")\n",
    ")\n",
    "\n",
    "(\n",
    "    ordersDF.orderBy(asc(\"users_id\"))\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .format(\"parquet\")\n",
    "    .bucketBy(8, \"users_id\")\n",
    "    .saveAsTable(\"OrdersTbl\")\n",
    ")\n",
    "\n",
    "spark.sql(\"cache table UsersTbl\")\n",
    "spark.sql(\"cache table OrdersTbl\")\n",
    "\n",
    "usersBucketDF = spark.table(\"UsersTbl\")\n",
    "ordersBucketDF = spark.table(\"OrdersTbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+-------+-----+-----+---+--------+-----------------------+----------+\n",
      "|transaction_id|quantity|users_id|amount |state|items|uid|login   |email                  |user_state|\n",
      "+--------------+--------+--------+-------+-----+-----+---+--------+-----------------------+----------+\n",
      "|4959          |4959    |2       |9918.0 |AZ   |SKU-4|2  |user_2  |user_2@databricks.com  |CA        |\n",
      "|1458          |1458    |2       |2916.0 |NY   |SKU-3|2  |user_2  |user_2@databricks.com  |CA        |\n",
      "|3986          |3986    |12      |7972.0 |CA   |SKU-5|12 |user_12 |user_12@databricks.com |AZ        |\n",
      "|9767          |9767    |28      |19534.0|CA   |SKU-1|28 |user_28 |user_28@databricks.com |TX        |\n",
      "|4833          |4833    |29      |9666.0 |MI   |SKU-1|29 |user_29 |user_29@databricks.com |CO        |\n",
      "|4368          |4368    |30      |8736.0 |CO   |SKU-2|30 |user_30 |user_30@databricks.com |NY        |\n",
      "|8507          |8507    |42      |17014.0|CO   |SKU-1|42 |user_42 |user_42@databricks.com |AZ        |\n",
      "|1132          |1132    |42      |2264.0 |AZ   |SKU-5|42 |user_42 |user_42@databricks.com |AZ        |\n",
      "|6506          |6506    |48      |13012.0|MI   |SKU-3|48 |user_48 |user_48@databricks.com |MI        |\n",
      "|5795          |5795    |48      |11590.0|TX   |SKU-1|48 |user_48 |user_48@databricks.com |MI        |\n",
      "|5228          |5228    |48      |10456.0|AZ   |SKU-5|48 |user_48 |user_48@databricks.com |MI        |\n",
      "|3683          |3683    |88      |7366.0 |CO   |SKU-1|88 |user_88 |user_88@databricks.com |CO        |\n",
      "|6931          |6931    |151     |13862.0|MI   |SKU-1|151|user_151|user_151@databricks.com|CO        |\n",
      "|1501          |1501    |151     |3002.0 |CO   |SKU-3|151|user_151|user_151@databricks.com|CO        |\n",
      "|5659          |5659    |156     |11318.0|CO   |SKU-5|156|user_156|user_156@databricks.com|AZ        |\n",
      "|9322          |9322    |171     |18644.0|AZ   |SKU-1|171|user_171|user_171@databricks.com|CO        |\n",
      "|4620          |4620    |171     |9240.0 |MI   |SKU-1|171|user_171|user_171@databricks.com|CO        |\n",
      "|9308          |9308    |201     |18616.0|AZ   |SKU-1|201|user_201|user_201@databricks.com|NY        |\n",
      "|2870          |2870    |208     |5740.0 |NY   |SKU-1|208|user_208|user_208@databricks.com|CA        |\n",
      "|9175          |9175    |210     |18350.0|MI   |SKU-4|210|user_210|user_210@databricks.com|TX        |\n",
      "+--------------+--------+--------+-------+-----+-----+---+--------+-----------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinUsersOrdersBucketDF = ordersBucketDF.join(usersBucketDF, ordersBucketDF.users_id == usersBucketDF.uid)\n",
    "joinUsersOrdersBucketDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When to use a shuffle sort merge join\n",
    "* 두 개의 큰 데이터 집합의 각 키를 정렬하고 동일한 파티션으로 해시 할 수 있는 경우\n",
    "* 정렬된 키들의 매칭을 기반으로 두 데이터집합 들을 결합하기 위해서, 동등 조인을 수행하기를 워한는 경우\n",
    "  - SortMerge 조인이기 때문에 이미 정렬된 상태의 Equi-join 이 가능하기 때문\n",
    "* 네트워크를 통해 대용량 셔플 파일을 저장 시에 Exchange 와 Sort 연산을 피하고 싶을 때\n",
    "  - Bucket 기법을 활용하는 예제를 고려하라는 말로 추측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7.4 Inspecting the Spark UI\n",
    "### 7.4.1 Journey Through the Spark UI Tabs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jobs and Stages\n",
    "> Duration 항목을 기준으로 문제가 되는 Job, Stage 및 Task 를 추측합니다\n",
    "\n",
    "* 확인 및 모니터링 대상 지표\n",
    "  - Average Duration 시간\n",
    "  - GC 에 소모되는 시간\n",
    "  - Shuffle bytes/records 정보\n",
    "\n",
    "![ch7-ui-1](images/ch7-ui-1.png)\n",
    "![ch7-ui-2](images/ch7-ui-2.png)\n",
    "\n",
    "![ch7-ui-3](images/ch7-ui-3.png)\n",
    "![ch7-ui-4](images/ch7-ui-4.png)\n",
    "![ch7-ui-5](images/ch7-ui-5.png)\n",
    "![ch7-ui-6](images/ch7-ui-6.png)\n",
    "![ch7-ui-7](images/ch7-ui-7.png)\n",
    "![ch7-ui-8](images/ch7-ui-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 추가로 학습할 내용들\n",
    "* [Tuning Apache Spark for Large Scale Workloads - Sital Kedia & Gaoxiang Liu](https://www.youtube.com/watch?v=5dga0UT4RI8)\n",
    "* [Hive Bucketing in Apache Spark - Tejas Patil](https://www.youtube.com/watch?v=6BD-Vv-ViBw)\n",
    "* [How does Facebook tune Apache Spark for Large-Scale Workloads?](https://towardsdatascience.com/how-does-facebook-tune-apache-spark-for-large-scale-workloads-3238ddda0830)\n",
    "* [External Shuffle Service in Apache Spark](https://www.waitingforcode.com/apache-spark/external-shuffle-service-apache-spark/read)\n",
    "* [Spark Internal Part 2. Spark의 메모리 관리(2)](https://medium.com/@leeyh0216/spark-internal-part-2-spark%EC%9D%98-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EA%B4%80%EB%A6%AC-2-db1975b74d2f)\n",
    "* [Why You Should Care about Data Layout in the Filesystem](https://databricks.com/session/why-you-should-care-about-data-layout-in-the-filesystem)\n",
    "* [Five distinct join strategies](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala#L111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
